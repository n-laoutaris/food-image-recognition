{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1c1aea8",
   "metadata": {},
   "source": [
    "# Final Ensemble Model Notebook\n",
    "\n",
    "This notebook demonstrates the process of generating predictions using an ensemble of the best individual models. The ensemble approach uses the strengths of multiple models to improve robustness and accuracy for multi-label food classification.\n",
    "\n",
    "**Key steps:**\n",
    "- Load and prepare the test dataset.\n",
    "- Load multiple trained models with their optimal thresholds and weights (based on the public leaderboard).\n",
    "- Generate predictions for each model, using our fallback mechanism to ensure at least one label per image.\n",
    "- Combine predictions using weighted voting.\n",
    "- Analyze model disagreements and influence.\n",
    "- Save the final submission.\n",
    "\n",
    "All code is structured for clarity and reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T16:25:43.955024Z",
     "start_time": "2025-05-14T16:25:43.282527Z"
    }
   },
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "import timm\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tabulate import tabulate\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Allow duplicate OpenMP libraries (fixes some multi-threading issues on some systems)\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2ecc3f",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "- **Device selection**: Automatically uses GPU if available, otherwise falls back to CPU.\n",
    "- **Directory paths**: Set up your paths to the test images. Please note this notebook assumes you also have all the necessary state dictionaries in the same directory as the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb649f2-f64f-4b8f-a862-eabf16236cb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T16:25:43.975891Z",
     "start_time": "2025-05-14T16:25:43.958026Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"CUDA Devices: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99231a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_test_dir = 'images_test'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c74513",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "- **Dataset class**: Handles both training and test data, with or without labels. \n",
    "- **Transforms**: Applies resizing and normalization to match model training.\n",
    "- **Dataloaders**: Efficiently loads test images in batches for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba613dbe24553df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T16:25:45.342367Z",
     "start_time": "2025-05-14T16:25:43.991807Z"
    }
   },
   "outputs": [],
   "source": [
    "class FoodDataset(Dataset):\n",
    "    def __init__(self, img_dir, labels_csv = None, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.is_training = labels_csv is not None # If labels_csv is provided, it's a training dataset. Else, it's a test dataset.\n",
    "\n",
    "        if self.is_training:\n",
    "            # Load training data\n",
    "            self.labels_df = pd.read_csv(labels_csv)      \n",
    "            self.filenames = self.labels_df.iloc[:, 0].values  # image filenames\n",
    "            self.labels = self.labels_df.iloc[:, 1:].values.astype('float')  # one-hot labels\n",
    "        else:\n",
    "            self.filenames = sorted(os.listdir(img_dir))\n",
    "            self.labels = None  # No labels for the test set\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Find the image file path and open it \n",
    "        img_path = os.path.join(self.img_dir, self.filenames[idx])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.is_training:\n",
    "            # For training data, return the image and its label\n",
    "            label = torch.tensor(self.labels[idx])\n",
    "            return image, label\n",
    "        else:\n",
    "            # For test data, return the image and its filename\n",
    "            return image, self.filenames[idx]  \n",
    "\n",
    "# A function to make a dataloader with the necessary transformations for the images according to what each model expects\n",
    "def make_test_dataloader(input_size):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((input_size, input_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5944, 0.5082, 0.4259], std=[0.2128, 0.2213, 0.2308])\n",
    "    ])\n",
    "    test_dataset = FoodDataset(img_dir=images_test_dir, transform=transform)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return test_dataloader\n",
    "\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09604da",
   "metadata": {},
   "source": [
    "## Model Loading and Configuration\n",
    "\n",
    "- **Model selection**: Loads a set of hand-picked models, each with its own architecture, input resolution, threshold, and ensemble weight.\n",
    "- **State dict handling**: Removes 'module.' prefix if present (from multi-GPU training).\n",
    "- **Model summary**: Prints a table summarizing all models in the ensemble, including their weights and thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51d4a4738a8e641",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T16:25:50.544208Z",
     "start_time": "2025-05-14T16:25:45.346698Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove 'module.' prefix from keys in state_dict. This happens when they were created in parallel gpu mode\n",
    "def remove_module_prefix(state_dict):\n",
    "    return {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "\n",
    "# This function will load all relevant model info into a dictionary, together with the model itself\n",
    "def load_model_info(filepath, model_type, resolution, threshold, raw_weight):\n",
    "    # Determine model class\n",
    "    if model_type == 'swinV1':\n",
    "        model = timm.create_model('swin_base_patch4_window7_224', pretrained=True, num_classes=498)\n",
    "    elif model_type == 'swinV2':\n",
    "        model = timm.create_model('swinv2_base_window12to16_192to256.ms_in22k_ft_in1k', pretrained=True, num_classes=498)\n",
    "    elif model_type == 'ViT':\n",
    "        model = timm.create_model('vit_base_mci_224.apple_mclip', pretrained=True, num_classes=498, drop_rate=0.1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "    # Make the correct dataloader based on what resolution the model expects\n",
    "    loader_test = make_test_dataloader(resolution)\n",
    "\n",
    "    # Load weights from state dict\n",
    "    state_dict = torch.load(filepath, map_location=device)\n",
    "    if any(k.startswith('module.') for k in state_dict.keys()):\n",
    "        state_dict = remove_module_prefix(state_dict)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"loader_test\": loader_test,\n",
    "        \"threshold\": threshold,\n",
    "        \"raw_weight\": raw_weight,\n",
    "        \"filename\": filepath,\n",
    "        \"model_type\": model_type,\n",
    "        \"resolution\": resolution\n",
    "    }\n",
    "\n",
    "# These are the configurations for the hand-picked models. Each tuple is: filepath, model type, input resolution, optimal threshold, weight (public leaderboard score)\n",
    "model_configs = [\n",
    "    ('SwinV1(4)_v16.pth', 'swinV1', 224, 0.38, 0.54353),\n",
    "    ('SwinV1(3-4)_v15.pth', 'swinV1', 224, 0.32, 0.56130),\n",
    "    ('SwinV1(3-4)_v18.pth', 'swinV1', 224, 0.32, 0.55537),\n",
    "    ('SwinV1(3-4)_v19.pth', 'swinV1', 224, 0.32, 0.54677),\n",
    "    ('SwinV1Billy.pth', 'swinV1', 224, 0.31, 0.54300), \n",
    "    ('SwinV2(4)_v17.pth', 'swinV2', 256, 0.38, 0.53080),\n",
    "    ('SwinV2(3-4)_v20.pth', 'swinV2', 256, 0.36, 0.53577),\n",
    "    ('ViT_0.54_sub_0.5_thresh.pth', 'ViT', 224, 0.5, 0.54756),\n",
    "    ('ViT_0.56_sub_0.5_thresh.pth', 'ViT', 224, 0.5, 0.56454),\n",
    "    ('vit_full_unfreeze_0.55589_sub.pth', 'ViT', 224, 0.5, 0.55589)\n",
    "    ]\n",
    "\n",
    "# Create the list of dictionaries that makes the ensemble and compute normalized weights\n",
    "models_info = [load_model_info(*cfg) for cfg in model_configs]\n",
    "total_weight = sum(m[\"raw_weight\"] for m in models_info)\n",
    "for m in models_info:\n",
    "    m[\"weight\"] = m[\"raw_weight\"] / total_weight\n",
    "\n",
    "# Print a summary table of the ensemble about to be put to work\n",
    "rows = []\n",
    "for m in models_info:\n",
    "    input_size = f\"{m['resolution']}x{m['resolution']}\"\n",
    "    rows.append([\n",
    "        m[\"model_type\"],\n",
    "        m[\"filename\"],\n",
    "        input_size,\n",
    "        m[\"threshold\"],\n",
    "        m[\"weight\"]\n",
    "    ])\n",
    "# Sort by normalized weight\n",
    "rows.sort(key=lambda x: x[4], reverse=True)\n",
    "# Format for display\n",
    "rows_display = [\n",
    "    [model_type, filename, input_size, f\"{threshold:.2f}\", f\"{weight:.4f}\"]\n",
    "    for model_type, filename, input_size, threshold, weight in rows\n",
    "]\n",
    "print(tabulate(rows_display, headers=[\"Model Type\", \"Filename\", \"Input Size\", \"Threshold\", \"Normalized Weight\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70c8599",
   "metadata": {},
   "source": [
    "## Ensemble Prediction Loop\n",
    "\n",
    "- **Per-model prediction**: Each model predicts on the test set using its optimal threshold.\n",
    "- **Fallback logic**: Ensures every image receives at least one label, even if all probabilities are below threshold. Reports how often fallback logic was needed across all models.\n",
    "- **Stacking predictions**: Combines all model predictions into a single tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a540b2d8e54e8620",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T16:59:12.368497Z",
     "start_time": "2025-05-14T16:58:39.510067Z"
    }
   },
   "outputs": [],
   "source": [
    "num_models = len(models_info)\n",
    "weights = torch.tensor([info[\"weight\"] for info in models_info])\n",
    "times_fallback = 0\n",
    "\n",
    "# To accumulate predictions and filenames \n",
    "all_preds_per_model = []\n",
    "ensemble_filenames = None\n",
    "\n",
    "for idx, info in enumerate(models_info):\n",
    "    model = info[\"model\"]\n",
    "    threshold = info[\"threshold\"]\n",
    "    loader_test = info[\"loader_test\"]\n",
    "\n",
    "    model_preds = []\n",
    "    model_filenames = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, filenames in tqdm(loader_test, desc=f\"Predicting with model {idx+1}/{num_models}\"):\n",
    "            X = X.to(device)\n",
    "            logits = model(X)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs > threshold).float() \n",
    "\n",
    "            # Fallback: Ensure at least one label per image based on individual model's predictions\n",
    "            for i in range(preds.size(0)):\n",
    "                if preds[i].sum() == 0:\n",
    "                    times_fallback += 1\n",
    "                    max_idx = torch.argmax(probs[i]).item()\n",
    "                    preds[i, max_idx] = 1\n",
    "\n",
    "            model_preds.append(preds.cpu())\n",
    "            model_filenames.extend(filenames)\n",
    "\n",
    "    model_preds = torch.cat(model_preds, dim=0)  # [N, C]\n",
    "    all_preds_per_model.append(model_preds)\n",
    "\n",
    "    if ensemble_filenames is None:\n",
    "        ensemble_filenames = model_filenames\n",
    "    else:\n",
    "        assert ensemble_filenames == model_filenames, \"Mismatch in test filenames across models!\"\n",
    "\n",
    "# Stack predictions from all models\n",
    "stacked_preds = torch.stack(all_preds_per_model)  # [num_models, num_samples, num_classes]\n",
    "\n",
    "print(f\"Total number of fallbacks (no labels set through thresholding): {times_fallback}/{1000*num_models}. Fallback rate: {100 * times_fallback / (1000 * num_models):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ae81cd",
   "metadata": {},
   "source": [
    "## Ensemble Conflict Analysis\n",
    "\n",
    "Measures the rate of disagreement between models for each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2ec4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conflict analysis\n",
    "vote_sum = stacked_preds.sum(dim=0)  # [N, C]\n",
    "conflicts = (vote_sum != 0) & (vote_sum != num_models)  # if everyone agrees we either have 0 or (num_models) in the sum\n",
    "conflict_votes = conflicts.sum().item()\n",
    "total_votes = conflicts.numel()\n",
    "percent = 100 * conflict_votes / total_votes\n",
    "print(f\"\\nTesting conflict rate: {percent:.2f}% ({conflict_votes:,}/{total_votes:,})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464f8cee",
   "metadata": {},
   "source": [
    "## Ensemble Influence Analysis\n",
    "\n",
    "To better understand the contribution of each model within our ensemble, we analyze their agreement and influence during the voting process. For every predicted label, we compute:\n",
    "\n",
    "- Agreement %: The proportion of times each model's prediction aligned with the final ensemble decision.\n",
    "- Decisive %: The proportion of times a model casts the decisive vote for the ensemble outcome.\n",
    "\n",
    "Note: This analysis does not evaluate whether that influence improved predictive performance or not. A model may sway the ensemble toward correct or incorrect labels. Therefore, decisiveness cannot be interpreted as a direct proxy for model quality without a ground-truth comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f53399f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_models, num_samples, num_classes = stacked_preds.shape\n",
    "\n",
    "# Use the same weights tensor \n",
    "weights = weights.to(stacked_preds.device)  \n",
    "# Compute the weighted ensemble decision\n",
    "weighted_votes = torch.einsum('mnc,m->nc', stacked_preds, weights)  # [num_samples, num_classes]\n",
    "ensemble_decision = (weighted_votes > 0.5).float()  \n",
    "\n",
    "# Influence tracking\n",
    "agree_with_vote = torch.zeros(num_models)\n",
    "decisive_votes = torch.zeros(num_models)\n",
    "\n",
    "for m in range(num_models):\n",
    "    model_votes = stacked_preds[m]                      # [num_samples, num_classes]\n",
    "    other_weights = torch.cat([weights[:m], weights[m+1:]])\n",
    "    other_models = torch.cat([stacked_preds[:m], stacked_preds[m+1:]], dim=0)  # [num_models-1, N, C]\n",
    "\n",
    "    # Agreement: how often model agrees with final ensemble decision\n",
    "    agree_with_vote[m] = (model_votes == ensemble_decision).sum().item()\n",
    "\n",
    "    # Decisiveness: remove model m and recompute weighted vote\n",
    "    new_weighted_votes = torch.einsum('mnc,m->nc', other_models, other_weights)\n",
    "    new_decision = (new_weighted_votes > 0.5).float()\n",
    "\n",
    "    # How often did removing the model flip the class prediction?\n",
    "    flipped_votes = (ensemble_decision != new_decision).float()\n",
    "    decisive_votes[m] = flipped_votes.sum().item()\n",
    "\n",
    "# Normalize to percentages\n",
    "total_votes = num_samples * num_classes\n",
    "agree_with_vote_pct = (agree_with_vote / total_votes) * 100\n",
    "decisive_votes_pct = (decisive_votes / total_votes) * 100\n",
    "\n",
    "# Make a dataframe and Display results\n",
    "data = []\n",
    "for m in range(num_models):\n",
    "    data.append({\n",
    "        \"Model\": models_info[m]['filename'],\n",
    "        \"Agreement %\": agree_with_vote_pct[m].item(),\n",
    "        \"Decisive %\": decisive_votes_pct[m].item()\n",
    "    })\n",
    "\n",
    "influence_df = pd.DataFrame(data)\n",
    "# sort by decisive power\n",
    "influence_df = influence_df.sort_values(by=\"Decisive %\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "influence_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff2f461",
   "metadata": {},
   "source": [
    "## Submission Generation\n",
    "\n",
    "- **Weighted voting**: Aggregates predictions using normalized model weights for robust final predictions.\n",
    "- **Submission file**: Saves the final ensemble predictions in the required CSV format for competition submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb35256",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_name = \"ensemble_submission_nik_v15.csv\"\n",
    "\n",
    "# Weighted majority voting (recomputed here in case we didn't want to run the influence analysis)\n",
    "weighted_preds = torch.einsum('mnc,m->nc', stacked_preds, weights)\n",
    "ensemble_preds = (weighted_preds > 0.5).int().numpy() # threshold is 0.5 because the weights are normalized\n",
    "\n",
    "#  Save Submission \n",
    "submission_df = pd.DataFrame(ensemble_preds, columns=[f\"label_{i}\" for i in range(ensemble_preds.shape[1])])\n",
    "submission_df.insert(0, \"Filename\", ensemble_filenames)\n",
    "submission_df.to_csv(submission_name, index=False)\n",
    "print(f\"Submission saved as {submission_name}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2510518",
   "metadata": {},
   "source": [
    "## Notes and Future Directions\n",
    "\n",
    "- We can extend this notebook to include further analysis, such as visualizing disagreements or exploring alternative ensembling strategies.\n",
    "- Adding a second level of fallback logic, applied after majority voting, reduced performance in our experiments. It is worth exploring more, however."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
